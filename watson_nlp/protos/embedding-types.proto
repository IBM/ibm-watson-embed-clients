/* ***************************************************************** */

/*                                                                   */

/* IBM Confidential                                                  */

/* OCO Source Materials                                              */

/*                                                                   */

/* (C) Copyright IBM Corp. 2001, 2020                                */

/*                                                                   */

/* The source code for this program is not published or otherwise    */

/* divested of its trade secrets, irrespective of what has been      */

/* deposited with the U. S. Copyright Office                         */

/*                                                                   */

/* US Government Users Restricted Rights - Use, duplication or       */

/* disclosure restricted by GSA ADP Schedule Contract with IBM Corp. */

/*                                                                   */

/* ***************************************************************** */



syntax = "proto3";

package watson_core_data_model.nlp;

option go_package = "github.ibm.com/ai-foundation/watson_nlp_runtime_client/watson_core_data_model/nlp";

option java_package = "com.ibm.watson.runtime.nlp";

option java_multiple_files = true;

import "matrix-types.proto";

import "producer-types.proto";



/***************************************************************************

 * This file houses protobuf interface definitions for embedding related   *

 * data structures. It is important to recognize that in Watson NLP,       *

 * embeddings are dense vectors and vectorizations are sparse vectors.     *

 * This the key distinction between the two, even though they seem very    *

 * related.                                                                *

 *                                                                         *

 * IMPORTANT: While they both wrap dense matrices, it is very important to *

 *            avoid conflating Embedding / EmbeddingPrediction. In general *

 *            you should think of the Embedding as a lookup table, usually *

 *            mapping the vocabulary to vectors. In contrast, an           *

 *            EmbeddingPrediction is the result of embedding an input text,*

 *            which may or may not be accomplished through application of  *

 *            the data matrix housed in Embedding. Not all NLP Embedding   *

 *            blocks will have a corresponding Embedding type, but in      *

 *            general, all of them will be able to leverage a model to     *

 *            generate EmbeddingPrediction objects.                        *

 **************************************************************************/



message Embedding {

    DenseMatrix data = 1; // Actual embedding matrix, e.g., word embedding vocab -> vec mat

    map<string, uint32> vocab_to_idx = 2; // Mapping from strings, e.g., n-grams -> row indices

    watson_core_data_model.common.ProducerId producer_id = 3; // (optional) The block that produced this embedding

    string pad_word = 4; // Padding word that was used in training.

    string unk_word = 5; // Unknown word that was used in training.

}



// Note: One day we may want to add spans to EmbeddingPredictions, but for now we do not.

message EmbeddingPrediction {

    DenseMatrix data = 1; // (required) result of embedding a text, e.g., n-grams

    repeated uint32 offsets = 2; // Offsets for batch prediction in stacked embedding objects

    watson_core_data_model.common.ProducerId producer_id = 3; // (optional) The block that produced this embedding prediction

}
